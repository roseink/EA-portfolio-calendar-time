{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d491ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "estimize_API_daily\n",
    "\n",
    "This file helps build and collect EA dates dataset \n",
    "moving forward in time, to complement the backtest files. \n",
    "\n",
    "Note that all tickers here may be backfilled. \n",
    "\n",
    "This code seeks to translates the following API query: \n",
    "curl -H \"X-Estimize-Key: API_KEY_HERE\" \n",
    "'api.estimize.com/estimates?start_date=2018-04-06&end_date=2018-04-07'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Package imports\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "import time\n",
    "from optparse import OptionParser\n",
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from optparse import OptionParser\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Setup for API\n",
    "url = \"http://api.estimize.com\"\n",
    "headers = {\"X-Estimize-Key\" : \"ca2bed82074413cd06d2711f\",\n",
    "\"content-type\" : \"application/json\"}\n",
    "\n",
    "# Define start date and get number of years between start of API and now\n",
    "start_date = date(2022, 1, 1)      # start this year! \n",
    "end_date = date.today() \n",
    "delta = end_date - start_date      # returns timedelta\n",
    "list_ticker_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9782be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>cusip</th>\n",
       "      <th>asofdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xoom Corporation</td>\n",
       "      <td>XOOM</td>\n",
       "      <td>98419Q101</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forrester Research Inc.</td>\n",
       "      <td>FORR</td>\n",
       "      <td>346563109</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cotiviti Holdings Inc.</td>\n",
       "      <td>COTV</td>\n",
       "      <td>22164K101</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eBay Inc.</td>\n",
       "      <td>EBAY</td>\n",
       "      <td>278642103</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ormat Technologies Inc.</td>\n",
       "      <td>ORA</td>\n",
       "      <td>686688102</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>SK Telecom Co., Ltd.</td>\n",
       "      <td>SKM</td>\n",
       "      <td>78440P306</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>MGM Resorts International</td>\n",
       "      <td>MGM</td>\n",
       "      <td>552953101</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>Twilio Inc</td>\n",
       "      <td>TWLO</td>\n",
       "      <td>90138F102</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>Uber Technologies, Inc.</td>\n",
       "      <td>UBER</td>\n",
       "      <td>90353T100</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>Yum! Brands, Inc.</td>\n",
       "      <td>YUM</td>\n",
       "      <td>988498101</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3323 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name ticker      cusip    asofdate\n",
       "0              Xoom Corporation   XOOM  98419Q101  2022-01-01\n",
       "1       Forrester Research Inc.   FORR  346563109  2022-01-01\n",
       "2        Cotiviti Holdings Inc.   COTV  22164K101  2022-01-01\n",
       "3                     eBay Inc.   EBAY  278642103  2022-01-01\n",
       "4       Ormat Technologies Inc.    ORA  686688102  2022-01-01\n",
       "...                         ...    ...        ...         ...\n",
       "3318       SK Telecom Co., Ltd.    SKM  78440P306  2022-01-01\n",
       "3319  MGM Resorts International    MGM  552953101  2022-01-01\n",
       "3320                 Twilio Inc   TWLO  90138F102  2022-01-01\n",
       "3321    Uber Technologies, Inc.   UBER  90353T100  2022-01-01\n",
       "3322          Yum! Brands, Inc.    YUM  988498101  2022-01-01\n",
       "\n",
       "[3323 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Query tickers first\n",
    "\n",
    "\"\"\"\n",
    "def get_tickers(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Overview: returns a dictionary of the company information for the company specified with ticker\n",
    "    Required params: None\n",
    "    Optional params: None\n",
    "    Response:\n",
    "\t-name: The name of the company\n",
    "\t-ticker: The ticker/symbol for the company\n",
    "    \"\"\"\n",
    "    function_url = url + \"/companies/%s\" % ticker\n",
    "    params = {\"start_date\" : start_date, \"end_date\" : end_date}\n",
    "    req = requests.get(function_url, params = params, headers=headers)\n",
    "    if req.status_code != 200 : return None\n",
    "    return req.json()\n",
    "\n",
    "# Loop over the date range, make sure to input one day at a time\n",
    "for i in range(delta.days + 1):\n",
    "    day = start_date + timedelta(days=i)\n",
    "    endday = day + timedelta(days = 1)\n",
    "    temp1 = pd.DataFrame(get_tickers(\"\", str(day), str(endday)))\n",
    "    temp1['asofdate'] = day\n",
    "    list_ticker_dfs.append(temp1)\n",
    "    \n",
    "# Get all unique tickers afterwards\n",
    "unique_tickers = pd.concat(list_ticker_dfs)\n",
    "unique_tickers = unique_tickers.drop_duplicates(['ticker'])\n",
    "unique_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a649db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>eps</th>\n",
       "      <th>revenue</th>\n",
       "      <th>consensus_eps_estimate</th>\n",
       "      <th>consensus_revenue_estimate</th>\n",
       "      <th>wallstreet_revenue_estimate</th>\n",
       "      <th>wallstreet_eps_estimate</th>\n",
       "      <th>consensus_weighted_revenue_estimate</th>\n",
       "      <th>consensus_weighted_eps_estimate</th>\n",
       "      <th>release_date</th>\n",
       "      <th>id</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>33.49</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-07-24T12:00:00-04:00</td>\n",
       "      <td>125752</td>\n",
       "      <td>XOOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>32.28</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-10-22T12:00:00-04:00</td>\n",
       "      <td>125753</td>\n",
       "      <td>XOOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>32.12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-02-04T11:00:00-05:00</td>\n",
       "      <td>125754</td>\n",
       "      <td>XOOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>35.94</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-04-22T12:00:00-04:00</td>\n",
       "      <td>125755</td>\n",
       "      <td>XOOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>39.84</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-07-22T12:00:00-04:00</td>\n",
       "      <td>125756</td>\n",
       "      <td>XOOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2045.83281</td>\n",
       "      <td>1.490096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-07T06:00:00-05:00</td>\n",
       "      <td>210118</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1636.00</td>\n",
       "      <td>1.097273</td>\n",
       "      <td>1637.8</td>\n",
       "      <td>1643.390199</td>\n",
       "      <td>1.092942</td>\n",
       "      <td>1638.284288</td>\n",
       "      <td>1.102351</td>\n",
       "      <td>2022-08-03T06:00:00-04:00</td>\n",
       "      <td>191944</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1868.714727</td>\n",
       "      <td>1.438375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-31T06:00:00-04:00</td>\n",
       "      <td>216586</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1547.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1623.857143</td>\n",
       "      <td>1591.551156</td>\n",
       "      <td>1.069789</td>\n",
       "      <td>1629.390063</td>\n",
       "      <td>1.143953</td>\n",
       "      <td>2022-05-04T06:00:00-04:00</td>\n",
       "      <td>188841</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1740.597644</td>\n",
       "      <td>1.301208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-01T06:00:00-04:00</td>\n",
       "      <td>214326</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191875 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fiscal_year  fiscal_quarter   eps  revenue consensus_eps_estimate  \\\n",
       "0         2013.0             2.0  0.14    33.49                   None   \n",
       "1         2013.0             3.0  0.06    32.28                   None   \n",
       "2         2013.0             4.0  0.06    32.12                   None   \n",
       "3         2014.0             1.0  0.06    35.94                   None   \n",
       "4         2014.0             2.0  0.09    39.84                   None   \n",
       "..           ...             ...   ...      ...                    ...   \n",
       "97        2023.0             4.0   NaN      NaN                    NaN   \n",
       "98        2022.0             2.0  1.05  1636.00               1.097273   \n",
       "99        2024.0             2.0   NaN      NaN                    NaN   \n",
       "100       2022.0             1.0  1.05  1547.00                   1.15   \n",
       "101       2024.0             1.0   NaN      NaN                    NaN   \n",
       "\n",
       "    consensus_revenue_estimate wallstreet_revenue_estimate  \\\n",
       "0                         None                         NaN   \n",
       "1                         None                         NaN   \n",
       "2                         None                         NaN   \n",
       "3                         None                         NaN   \n",
       "4                         None                         NaN   \n",
       "..                         ...                         ...   \n",
       "97                         NaN                  2045.83281   \n",
       "98                      1637.8                 1643.390199   \n",
       "99                         NaN                 1868.714727   \n",
       "100                1623.857143                 1591.551156   \n",
       "101                        NaN                 1740.597644   \n",
       "\n",
       "    wallstreet_eps_estimate consensus_weighted_revenue_estimate  \\\n",
       "0                     -0.07                                None   \n",
       "1                     -0.01                                None   \n",
       "2                     -0.01                                None   \n",
       "3                     -0.04                                None   \n",
       "4                      0.05                                None   \n",
       "..                      ...                                 ...   \n",
       "97                 1.490096                                 NaN   \n",
       "98                 1.092942                         1638.284288   \n",
       "99                 1.438375                                 NaN   \n",
       "100                1.069789                         1629.390063   \n",
       "101                1.301208                                 NaN   \n",
       "\n",
       "    consensus_weighted_eps_estimate               release_date      id ticker  \n",
       "0                              None  2013-07-24T12:00:00-04:00  125752   XOOM  \n",
       "1                              None  2013-10-22T12:00:00-04:00  125753   XOOM  \n",
       "2                              None  2014-02-04T11:00:00-05:00  125754   XOOM  \n",
       "3                              None  2014-04-22T12:00:00-04:00  125755   XOOM  \n",
       "4                              None  2014-07-22T12:00:00-04:00  125756   XOOM  \n",
       "..                              ...                        ...     ...    ...  \n",
       "97                              NaN  2024-02-07T06:00:00-05:00  210118    YUM  \n",
       "98                         1.102351  2022-08-03T06:00:00-04:00  191944    YUM  \n",
       "99                              NaN  2024-07-31T06:00:00-04:00  216586    YUM  \n",
       "100                        1.143953  2022-05-04T06:00:00-04:00  188841    YUM  \n",
       "101                             NaN  2024-05-01T06:00:00-04:00  214326    YUM  \n",
       "\n",
       "[191875 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Look through exchange tickers to get event id's \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_release_id(ticker):\n",
    "    \"\"\"\n",
    "    Overview: returns a list of dictionaries of the past financial releases for the speficied company (by ticker)\n",
    "    Required params: None\n",
    "    Optional params: None\n",
    "    Response: \n",
    "\t-fiscal_year: The fiscal year for the release\n",
    "\t-fiscal_quarter: The fiscal quarter for the release\n",
    "\t-release_date: The date of the release\n",
    "\t-eps: The earnings per share for the specified fiscal quarter\n",
    "\t-revenue: The revenue for the speified fiscal quarter\n",
    "\t-wallstreet_eps_estimate: The estimated EPS from Wall Street\n",
    "\t-wallstreet_revenue_estimate: The estimated revenue from Wall Street\n",
    "\t-consensus_eps_estimate: The average estimated EPS by the Estimize community \n",
    "\t-consensus_revenue_estimate: The average estimated revenue by the Estimize community\n",
    "    \"\"\"\n",
    "    function_url = url + \"/companies/%s/releases\" % ticker\n",
    "    req = requests.get(function_url, headers=headers)\n",
    "    if req.status_code != 200 : return None\n",
    "    return pd.DataFrame(req.json())\n",
    "\n",
    "# Save information (fiscal year-quarter, final estimates, id and release date)\n",
    "list_release_ids = []\n",
    "\n",
    "# Get release id's for unique tickers from above \n",
    "for i in range(len(unique_tickers)):\n",
    "    temp2 = pd.DataFrame(get_release_id(unique_tickers.ticker[i]))\n",
    "    temp2['ticker'] = unique_tickers.ticker[i]\n",
    "    list_release_ids.append(temp2)\n",
    "    \n",
    "# Join together in data frame \n",
    "tics_event_ids = pd.concat(list_release_ids)\n",
    "tics_event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the statement: curl -H \"X-Estimize-Key: ca2bed82074413cd06d2711f\"\n",
    "url = \"http://api.estimize.com\"\n",
    "headers = {\"X-Estimize-Key\" : \"ca2bed82074413cd06d2711f\", \n",
    "           \"content-type\" : \"application/json\", \"accept-encoding\": \"gzip, deflate\",}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "reinvestigate_ws = pd.DataFrame()\n",
    "\n",
    "# API Request for Wall Street Estimates \n",
    "def get_ws_est_eps_hist(ticker, release_id):\n",
    "\n",
    "    trials = 0\n",
    "\n",
    "    # Call API or return an empty df if 3 retries fail\n",
    "    try:\n",
    "        function_url = url + \"/releases/%s/consensus?type=wallstreet\" % release_id\n",
    "        req = requests.get(function_url, headers=headers)\n",
    "    \n",
    "        req.encoding = 'utf-8'\n",
    "        status_code = req.status_code\n",
    "    except:\n",
    "        print(\"Hit an issue.\")\n",
    "        req = None\n",
    "        status_code = None\n",
    "    while status_code != 200 or req is None:\n",
    "        trials = trials + 1\n",
    "        print(\"Rerun attempt \", trials, sep=\"\")\n",
    "        time.sleep(60) # sleeps for one minute and attempts another API call\n",
    "        try:\n",
    "            req = requests.get(function_url, headers=headers, timeout=10)\n",
    "            req.encoding = 'utf-8'\n",
    "        except:\n",
    "            print(\"rerun attempt failed.\")\n",
    "        if trials == 3:\n",
    "            print(\"The API request was attempted 3 times.\")\n",
    "            df = pd.DataFrame()\n",
    "            return df\n",
    "            break\n",
    "\n",
    "    # create a dataframe from json object\n",
    "    try:\n",
    "        df = pd.DataFrame(req.json()['wallstreet']['eps']['revisions'])\n",
    "    except:\n",
    "        print(\"There was an issue with the output format.\")\n",
    "        try:\n",
    "            response = req.text.strip()\n",
    "            print(response[-1])\n",
    "            response2 = response[:-1] + \"]\"\n",
    "            print(response2[-1])\n",
    "            response3 = json.loads(response2)\n",
    "            df = pd.DataFrame(response3)\n",
    "            print(df)      # these data frames need to be re-investigated \n",
    "            # Save these to be reinvestigated dataframe \n",
    "            row = pd.DataFrame()\n",
    "            row['ticker'] = str(ticker)\n",
    "            row['release_id'] = release_id\n",
    "            reinvestigate_ws = pd.concat([reinvestigate_ws, row], axis=0)\n",
    "\n",
    "        except:\n",
    "            print(\"The output format issue was not resolved.\")\n",
    "            df = pd.DataFrame()\n",
    "            fname = str(ticker) + \"_and_\" + str(release_id) + \"_ws\" + \".txt\" \n",
    "            with open(fname, \"w\") as f:\n",
    "                f.write(req.text)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ouptut dir name -- CHANGE IF NEEDED \n",
    "#output_dir = \"/kellogg/proj/mrl898/estimize/output\"\n",
    "output_dir = \"C://Users/clj585/Downloads/estimizetemp\"\n",
    "\n",
    "# rerun filename\n",
    "rerun_file = \"rerun_ws.csv\"\n",
    "\n",
    "# empty dataframes for reruns\n",
    "rerunList = []\n",
    "initial_run = []\n",
    "\n",
    "# initial count\n",
    "count = 0\n",
    "\n",
    "# Loop through these ticker-event ID pairings \n",
    "for index, row in tics_event_ids.iterrows():\n",
    "    \n",
    "    # print the request count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "    # print row's ticker and event ID\n",
    "    ticker_id_str = str(row['ticker']) + \"_and_\" + str(row['id'])\n",
    "    print(ticker_id_str)\n",
    "    \n",
    "    # send the API request\n",
    "    output = get_ws_est_eps_hist(row['ticker'], row['id'])\n",
    "\n",
    "    # change directories to output\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    # if the output dataframe is empty save it to rerun the date range later\n",
    "    if output.empty:\n",
    "\n",
    "        print(\"No output file for the following: \", ticker_id_str, sep=\"\")\n",
    "\n",
    "        # save to rerun dataframe\n",
    "        \n",
    "        rerunList.append(ticker_id_str) # save the missing date range\n",
    "        initial_run.append(datetime.now()) # save the current run time\n",
    "\n",
    "        df_rerun = {'missing ticker-id pair':rerunList,\n",
    "                    'initial run time':initial_run} # create a dict\n",
    "        df_rerun = pd.DataFrame(df_rerun) # create a dataframe\n",
    "        file_exists = os.path.isfile(rerun_file) # see if the rerun file exits\n",
    "\n",
    "        if not file_exists:\n",
    "            df_rerun.to_csv(rerun_file, index=False)\n",
    "        else:\n",
    "            df_rerun.to_csv(rerun_file, mode='a', header=False, index=False)\n",
    "\n",
    "    else:\n",
    "        # save output dataframe -- CAN CHANGE TO PARQUET IF LIKE \n",
    "        output_name = str(ticker_id_str) + \"_ws\" + \".csv\"\n",
    "        output['ticker'] = row['ticker']\n",
    "        output['release_id'] = row['id']\n",
    "        output.to_csv(output_name, index=False)\n",
    "\n",
    "    # sleep time 5 seconds\n",
    "    time.sleep(5)\n",
    "    \n",
    "# Reinvestigate these and deal with these separately... \n",
    "reinvestigate_ws.to_csv(output_dir + \"/reinvestigate_ws.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff03ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinvestigate_cons = pd.DataFrame()\n",
    "\n",
    "# API Request for consensus! \n",
    "def get_cons_est_eps_hist(ticker, release_id):\n",
    "\n",
    "    trials = 0\n",
    "\n",
    "    # Call API or return an empty df if 3 retries fail\n",
    "    try:\n",
    "        function_url = url + \"/releases/%s/consensus?type=estimize\" % release_id\n",
    "        req = requests.get(function_url, headers=headers)\n",
    "    \n",
    "        req.encoding = 'utf-8'\n",
    "        status_code = req.status_code\n",
    "    except:\n",
    "        print(\"Hit an issue.\")\n",
    "        req = None\n",
    "        status_code = None\n",
    "    while status_code != 200 or req is None:\n",
    "        trials = trials + 1\n",
    "        print(\"Rerun attempt \", trials, sep=\"\")\n",
    "        time.sleep(60) # sleeps for one minute and attempts another API call\n",
    "        try:\n",
    "            req = requests.get(function_url, headers=headers, timeout=10)\n",
    "            req.encoding = 'utf-8'\n",
    "        except:\n",
    "            print(\"rerun attempt failed.\")\n",
    "        if trials == 3:\n",
    "            print(\"The API request was attempted 3 times.\")\n",
    "            df = pd.DataFrame()\n",
    "            return df\n",
    "            break\n",
    "\n",
    "    # create a dataframe from json object\n",
    "    try:\n",
    "        df = pd.DataFrame(req.json()['estimize_weighted']['eps']['revisions'])\n",
    "    except:\n",
    "        print(\"There was an issue with the output format.\")\n",
    "        try:\n",
    "            response = req.text.strip()\n",
    "            print(response[-1])\n",
    "            response2 = response[:-1] + \"]\"\n",
    "            print(response2[-1])\n",
    "            response3 = json.loads(response2)\n",
    "            df = pd.DataFrame(response3)\n",
    "            print(df)   # these data frames need to be re-investigated \n",
    "            # Save these to be reinvestigated dataframe \n",
    "            row = pd.DataFrame()\n",
    "            row['ticker'] = str(ticker)\n",
    "            row['release_id'] = release_id\n",
    "            reinvestigate_cons = pd.concat([reinvestigate_cons, row], axis=0)\n",
    "        except:\n",
    "            print(\"The output format issue was not resolved.\")\n",
    "            df = pd.DataFrame()\n",
    "            fname = str(ticker) + \"_and_\" + str(release_id) + \"_cons\" + \".txt\" \n",
    "            with open(fname, \"w\") as f:\n",
    "                f.write(req.text)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ouptut dir name\n",
    "#output_dir = \"/kellogg/proj/mrl898/estimize/output\"\n",
    "output_dir = \"C://Users/clj585/Downloads/estimizetemp\"\n",
    "\n",
    "# rerun filename\n",
    "rerun_file = \"rerun_cons.csv\"\n",
    "\n",
    "# empty dataframes for reruns\n",
    "rerunList = []\n",
    "initial_run = []\n",
    "\n",
    "# initial count\n",
    "count = 0\n",
    "\n",
    "# Loop through these ticker-event ID pairings \n",
    "for index, row in tics_event_ids.iterrows():\n",
    "    \n",
    "    # print the request count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "    # print row's ticker and event ID\n",
    "    ticker_id_str = str(row['ticker']) + \"_and_\" + str(row['id'])\n",
    "    print(ticker_id_str)\n",
    "    \n",
    "    # send the API request\n",
    "    output = get_cons_eps_hist(row['ticker'], row['id'])\n",
    "    \n",
    "    # change directories to output\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    # if the output dataframe is empty save it to rerun the date range later\n",
    "    if output.empty:\n",
    "\n",
    "        print(\"No output file for this date range: \", ticker_id_str, sep=\"\")\n",
    "\n",
    "        # save to rerun dataframe\n",
    "        \n",
    "        rerunList.append(ticker_id_str) # save the missing date range\n",
    "        initial_run.append(datetime.now()) # save the current run time\n",
    "\n",
    "        df_rerun = {'missing ticker-id pair':rerunList,\n",
    "                    'initial run time':initial_run} # create a dict\n",
    "        df_rerun = pd.DataFrame(df_rerun) # create a dataframe\n",
    "        file_exists = os.path.isfile(rerun_file) # see if the rerun file exits\n",
    "\n",
    "        if not file_exists:\n",
    "            df_rerun.to_csv(rerun_file, index=False)\n",
    "        else:\n",
    "            df_rerun.to_csv(rerun_file, mode='a', header=False, index=False)\n",
    "\n",
    "    else:\n",
    "        # save output dataframe\n",
    "        output_name = str(ticker_id_str) + \"_cons\" + \".csv\"\n",
    "        output['ticker'] = row['ticker']\n",
    "        output['release_id'] = row['id']\n",
    "        output.to_csv(output_name, index=False)\n",
    "\n",
    "    # sleep time 5 seconds\n",
    "    time.sleep(5)\n",
    "    \n",
    "# Save reinvestigate dataframe and examine those JSONS\n",
    "reinvestigate_cons.to_csv(output_dir + \"/reinvestigate_cons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinvestigate_wgt = pd.DataFrame()\n",
    "\n",
    "# API Request Method for weighted! \n",
    "def get_wgt_est_eps_hist(ticker, release_id):\n",
    "\n",
    "    trials = 0\n",
    "    # Call API or return an empty df if 3 retries fail\n",
    "    try:\n",
    "        function_url = url + \"/releases/%s/consensus?type=estimize_weighted\" % release_id\n",
    "        req = requests.get(function_url, headers=headers)\n",
    "    \n",
    "        req.encoding = 'utf-8'\n",
    "        status_code = req.status_code\n",
    "    except:\n",
    "        print(\"Hit an issue.\")\n",
    "        req = None\n",
    "        status_code = None\n",
    "    while status_code != 200 or req is None:\n",
    "        trials = trials + 1\n",
    "        print(\"Rerun attempt \", trials, sep=\"\")\n",
    "        time.sleep(60) # sleeps for one minute and attempts another API call\n",
    "        try:\n",
    "            req = requests.get(function_url, headers=headers, timeout=10)\n",
    "            req.encoding = 'utf-8'\n",
    "        except:\n",
    "            print(\"rerun attempt failed.\")\n",
    "        if trials == 3:\n",
    "            print(\"The API request was attempted 3 times.\")\n",
    "            df = pd.DataFrame()\n",
    "            return df\n",
    "            break\n",
    "\n",
    "    # create a dataframe from json object\n",
    "    try:\n",
    "        df = pd.DataFrame(req.json()['estimize']['eps']['revisions'])\n",
    "    except:\n",
    "        print(\"There was an issue with the output format.\")\n",
    "        try:\n",
    "            response = req.text.strip()\n",
    "            print(response[-1])\n",
    "            response2 = response[:-1] + \"]\"\n",
    "            print(response2[-1])\n",
    "            response3 = json.loads(response2)\n",
    "            df = pd.DataFrame(response3)\n",
    "            print(df)  # these data frames need to be re-investigated \n",
    "            # Save these to be reinvestigated dataframe \n",
    "            row = pd.DataFrame()\n",
    "            row['ticker'] = str(ticker)\n",
    "            row['release_id'] = release_id\n",
    "            reinvestigate_wgt = pd.concat([reinvestigate_wgt, row], axis=0)\n",
    "        except:\n",
    "            print(\"The output format issue was not resolved.\")\n",
    "            df = pd.DataFrame()\n",
    "            fname = str(ticker) + \"_and_\" + str(release_id) + \"_wgt\" + \".txt\" \n",
    "            with open(fname, \"w\") as f:\n",
    "                f.write(req.text)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ouptut dir name\n",
    "#output_dir = \"/kellogg/proj/mrl898/estimize/output\"\n",
    "output_dir = \"C://Users/clj585/Downloads/estimizetemp\"\n",
    "\n",
    "# rerun filename\n",
    "rerun_file = \"rerun_wgt.csv\"\n",
    "\n",
    "# empty dataframes for reruns\n",
    "rerunList = []\n",
    "initial_run = []\n",
    "\n",
    "# initial count\n",
    "count = 0\n",
    "\n",
    "# Loop through these ticker-event ID pairings \n",
    "for index, row in tics_event_ids.iterrows():\n",
    "    \n",
    "    # print the request count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "    # print row's ticker and event ID\n",
    "    ticker_id_str = str(row['ticker']) + \"_and_\" + str(row['id'])\n",
    "    print(ticker_id_str)\n",
    "  \n",
    "    # send the API request\n",
    "    output = get_wgt_eps_hist(row['ticker'], row['id'])\n",
    "\n",
    "    # change directories to output\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    # if the output dataframe is empty save it to rerun the date range later\n",
    "    if output.empty:\n",
    "\n",
    "        print(\"No output file for this date range: \", ticker_id_str, sep=\"\")\n",
    "\n",
    "        # save to rerun dataframe\n",
    "        \n",
    "        rerunList.append(ticker_id_str) # save the missing date range\n",
    "        initial_run.append(datetime.now()) # save the current run time\n",
    "\n",
    "        df_rerun = {'missing ticker-id pair':rerunList,\n",
    "                    'initial run time':initial_run} # create a dict\n",
    "        df_rerun = pd.DataFrame(df_rerun) # create a dataframe\n",
    "        file_exists = os.path.isfile(rerun_file) # see if the rerun file exits\n",
    "\n",
    "        if not file_exists:\n",
    "            df_rerun.to_csv(rerun_file, index=False)\n",
    "        else:\n",
    "            df_rerun.to_csv(rerun_file, mode='a', header=False, index=False)\n",
    "\n",
    "    else:\n",
    "        # save output dataframe\n",
    "        output_name = str(ticker_id_str) + \"_wgt\" + \".csv\"\n",
    "        output['ticker'] = row['ticker']\n",
    "        output['release_id'] = row['id']\n",
    "        output.to_csv(output_name, index=False)\n",
    "\n",
    "    # sleep time 5 seconds\n",
    "    time.sleep(5)\n",
    "    \n",
    "# Reinvestigate these JSONS \n",
    "reinvestigate_wgt.to_csv(output_dir + \"/reinvestigate_wgt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94baf44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
